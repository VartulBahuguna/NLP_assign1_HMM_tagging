{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\VARTUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\VARTUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add start and end tags:\n",
    "def add_start_end_tag(sentences_original):\n",
    "    sentences=[\"\"]*len(sentences_original)\n",
    "    for i in range(len(sentences_original)):\n",
    "        sentences[i]=[(\"<start>\",\"<start>\")]+sentences_original[i]+[(\"<end>\",\"<end>\")]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(sentence):\n",
    "    new_sentence=[]\n",
    "    for i in sentence:\n",
    "        new_sentence.append(i[0])\n",
    "\n",
    "    # print(new_sentence)\n",
    "\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits data in k sets:\n",
    "def k_splits (k, data):\n",
    "    splits = {}\n",
    "    n = len(data)//k\n",
    "    for i in range(0, k):\n",
    "        i = int(i)\n",
    "        if(i+1 == k):\n",
    "            splits[i]= data[n*i : ]\n",
    "        else:\n",
    "            splits[i]=data[n*i : n*(i+1)]\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(confusion_matrix):\n",
    "    precision = []\n",
    "    n = len(confusion_matrix)\n",
    "    for i in range(0,n):\n",
    "        sum = 0\n",
    "        for j in range(0,len(confusion_matrix[i])):\n",
    "            sum+=confusion_matrix[j][i]\n",
    "        precision.append(confusion_matrix[i][i]/sum)\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(confusion_matrix):\n",
    "    recall = []\n",
    "    n = len(confusion_matrix)\n",
    "    for i in range(0,n):\n",
    "        sum = 0\n",
    "        for j in range(0,len(confusion_matrix[i])):\n",
    "            sum+=confusion_matrix[i][j]\n",
    "        recall.append(confusion_matrix[i][i]/sum)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calculator (confusion_matrix):\n",
    "    actual=0\n",
    "    count=0\n",
    "    for i in range(0,len(confusion_matrix)):\n",
    "        actual+=confusion_matrix[i][i]\n",
    "        count+=sum(confusion_matrix[i])\n",
    "    return (actual/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(confusion_matrix):\n",
    "\n",
    "    # Example data\n",
    "    data = confusion_matrix/np.sum(confusion_matrix,axis=1)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.imshow(data,  cmap='Spectral', interpolation='none')\n",
    "\n",
    "    # Add a colorbar\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add labels (optional)\n",
    "    plt.title('Heatmap Example')\n",
    "    plt.xlabel('X axis')\n",
    "    plt.ylabel('Y axis')\n",
    "    # Display the plot\n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_model:\n",
    "    def __init__(self):\n",
    "        self.confusion_matrix=None\n",
    "        self.transition_matrix=None\n",
    "        self.emission_matrix=None\n",
    "        self.tags_to_idx=None\n",
    "        self.count_of_each_tag=None\n",
    "        self.idx_to_tag=None\n",
    "        self.word_to_idx=None\n",
    "        self.idx_to_word=None\n",
    "    \n",
    "    def creat_tags_meta(self,sentences):\n",
    "        \n",
    "        #tag_to_idx: given a tag, what is its index : tags dict {\"tag\" : \"tag_index\"}\n",
    "        #idx_to_tag: given an index what is the tag : tags_reverse list of string\n",
    "        #count_of_each_tag: count of each tag\n",
    "        self.tag_to_idx={}\n",
    "        self.idx_to_tag=[]\n",
    "        cnt = 0\n",
    "        self.count_of_each_tag=[]\n",
    "        for i in range(len(sentences)):\n",
    "            for j in sentences[i]:\n",
    "                if j[1] not in self.tag_to_idx:\n",
    "                    self.tag_to_idx[j[1]] = cnt\n",
    "                    self.idx_to_tag.append(j[1])\n",
    "                    self.count_of_each_tag.append(0)\n",
    "                    cnt += 1\n",
    "                self.count_of_each_tag[self.tag_to_idx[j[1]]]+=1\n",
    "\n",
    "\n",
    "    def create_word_idx_translation(self,sentences):\n",
    "        #word to index number\n",
    "        #word_to_idx is word to index number\n",
    "        #idx_to_word\n",
    "        \n",
    "        idx = 0\n",
    "        self.word_to_idx={}\n",
    "        self.idx_to_word = []\n",
    "        # word_to_idx[\"<start>\"]=idx\n",
    "        for sentence in sentences:\n",
    "            for tup in sentence:\n",
    "                if tup[0] not in self.word_to_idx:\n",
    "                    \n",
    "                    self.word_to_idx[tup[0]]=idx\n",
    "                    idx += 1\n",
    "                    self.idx_to_word.append(tup[0])\n",
    "        self.word_to_idx[\"<unknown>\"]=idx\n",
    "        self.idx_to_word.append(\"<unknown>\")\n",
    "\n",
    "        # print('idx--------------------------------', idx)\n",
    "        # print('worddict--------------------------------', len(word_to_idx))\n",
    "\n",
    "\n",
    "    def create_transition_matrix(self,sentences):\n",
    "        #creating transition matrix\n",
    "        #bigram_matrix : count of every bigram tags\n",
    "        #transmission_matrix : probability of every bigram tags \n",
    "        self.bigram_matrix = []\n",
    "        self.transition_matrix = []\n",
    "        total_tags = len(self.count_of_each_tag)\n",
    "        for i in range(total_tags): \n",
    "            self.bigram_matrix.append([0]*total_tags)\n",
    "            self.transition_matrix.append([0]*total_tags)\n",
    "\n",
    "        #creating a bigram matrix\n",
    "        for sentence in sentences:\n",
    "            for j in range(len(sentence)-1):\n",
    "                self.bigram_matrix[self.tag_to_idx[sentence[j][1]]][self.tag_to_idx[sentence[j+1][1]]]+=1\n",
    "        # print(bigram_matrix)\n",
    "        \n",
    "        #create transition matrix A\n",
    "        for i in range(total_tags):\n",
    "            for j in range(total_tags):\n",
    "                self.transition_matrix[i][j]=(self.bigram_matrix[i][j]+1)/(self.count_of_each_tag[i] + len(self.count_of_each_tag))  #adding 1 to numerator and total unique tags in denominator for smoothing(Laplace)\n",
    "        # print(transition_matrix)\n",
    "\n",
    "\n",
    "    def create_emission_matrix(self,sentences):\n",
    "        #emission_matrix : given a word, what is the probability of tag\n",
    "        #emission_count_matrix : given a word, what is the count of each tag that it can have\n",
    "        # words = list(brown.words())\n",
    "        words_cnt = len(self.word_to_idx)\n",
    "        total_tags = len(self.tag_to_idx)\n",
    "        self.emission_matrix = []\n",
    "        \n",
    "        for i in range(words_cnt):\n",
    "            self.emission_matrix.append([0]*total_tags)\n",
    "        # print(\"--------------------------------------\",len(word_to_idx))\n",
    "        # print(\"------------------------\", len(emission_matrix))\n",
    "        # print(\"------------------------\", word_to_idx['<unknown>'])\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for j in sentence:\n",
    "                self.emission_matrix[self.word_to_idx[j[0]]][self.tag_to_idx[j[1]]]+=1\n",
    "\n",
    "        emission_count_matrix = self.emission_matrix\n",
    "\n",
    "        for i in range(words_cnt):\n",
    "            for j in range(total_tags):\n",
    "                self.emission_matrix[i][j] = (self.emission_matrix[i][j]+1)/(self.count_of_each_tag[j] + words_cnt)\n",
    "        # print(\"------------------------\", emission_matrix[word_to_idx['unknown'])\n",
    "\n",
    "        # handling missing case\n",
    "        aux=[0]*len(self.tag_to_idx.keys())\n",
    "        # print(tag_to_idx)\n",
    "        aux[self.tag_to_idx['NOUN']]=self.count_of_each_tag[self.tag_to_idx['NOUN']]/total_tags\n",
    "        aux[self.tag_to_idx['VERB']]=self.count_of_each_tag[self.tag_to_idx['VERB']]/total_tags\n",
    "        aux[self.tag_to_idx['ADV']]=self.count_of_each_tag[self.tag_to_idx['ADV']]/total_tags\n",
    "        aux[self.tag_to_idx['ADJ']]=self.count_of_each_tag[self.tag_to_idx['ADJ']]/total_tags\n",
    "        # print(aux)\n",
    "        self.emission_matrix[self.word_to_idx['<unknown>']]=aux\n",
    "\n",
    "\n",
    "    def build_confusion_matrix(self,actual,predicted):\n",
    "        #cm: confusion matrix\n",
    "        for i in range(len(actual)):\n",
    "            self.confusion_matrix[self.tag_to_idx[actual[i][1]]][self.tag_to_idx[predicted[i]]]+=1\n",
    "\n",
    "    def HMM_logic(self,input_sentence):\n",
    "        # print(tags_dict)\n",
    "        tags_output=[\"<start>\"]\n",
    "        last_prob = 1\n",
    "        viterbi=[]\n",
    "        for i in range(len(self.tag_to_idx)):\n",
    "            viterbi.append(([0])*len(input_sentence))\n",
    "        \n",
    "        viterbi[self.tag_to_idx[\"<start>\"]][0]=1\n",
    "        \n",
    "        for i in range(1, len(input_sentence)):\n",
    "            word_prob=0\n",
    "            tags=\"\"\n",
    "            curr_ob = input_sentence[i]\n",
    "            if curr_ob not in self.word_to_idx:\n",
    "                curr_ob=\"<unknown>\"\n",
    "            for curr_tag in self.tag_to_idx:\n",
    "                max_prob=0\n",
    "                for prev_tag in self.tag_to_idx:\n",
    "                    temp = self.transition_matrix[self.tag_to_idx[prev_tag]][self.tag_to_idx[curr_tag]]*self.emission_matrix[self.word_to_idx[curr_ob]][self.tag_to_idx[curr_tag]]*viterbi[self.tag_to_idx[prev_tag]][i-1]\n",
    "                    if temp>max_prob:\n",
    "                        max_prob=temp\n",
    "                viterbi[self.tag_to_idx[curr_tag]][i]=max_prob\n",
    "                if max_prob>word_prob:\n",
    "                    word_prob=max_prob\n",
    "                    tags=curr_tag\n",
    "            if tags=='':\n",
    "                tags='X'\n",
    "            tags_output.append(tags)\n",
    "\n",
    "        return tags_output  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self,sentences_original, k):\n",
    "\n",
    "        model=HMM_model()\n",
    "\n",
    "        sentences = add_start_end_tag(sentences_original)\n",
    "        \n",
    "        #spliting data into k splits\n",
    "        cross_validation_set=k_splits(k,sentences)\n",
    "        keys=list(cross_validation_set.keys())\n",
    "\n",
    "        #creating master meta data i.e. over whole corpus\n",
    "        self.creat_tags_meta(sentences)\n",
    "        self.create_word_idx_translation(sentences)\n",
    "        #instializing master transition, emission and confusion matrices\n",
    "        master_transition_matrix = []\n",
    "        master_emission_matrix = []\n",
    "        master_confusion_matrix = []\n",
    "\n",
    "        for i in range(len(self.count_of_each_tag)):\n",
    "            master_transition_matrix.append([0]*len(self.count_of_each_tag))\n",
    "            master_confusion_matrix.append([0]*len(self.count_of_each_tag))\n",
    "\n",
    "        for i in range(len(self.word_to_idx)):\n",
    "            master_emission_matrix.append([0]*len(self.count_of_each_tag))\n",
    "\n",
    "\n",
    "\n",
    "        #model building and traing on k sets\n",
    "        for i in keys:\n",
    "\n",
    "            #**********************------------------create test and train------------------************************************\n",
    "            train_data=[]\n",
    "            for j in cross_validation_set.keys():\n",
    "                if j!=i:\n",
    "                    train_data+=cross_validation_set[j]\n",
    "\n",
    "            test_data=cross_validation_set[i]\n",
    "            #**********************---------------------------------------************************************\n",
    "            \n",
    "\n",
    "            #**********************---------------create tags and words meta data------------------------************************************\n",
    "            model.creat_tags_meta(train_data)\n",
    "            model.create_word_idx_translation(train_data)\n",
    "            #**********************---------------------------------------************************************\n",
    "\n",
    "\n",
    "            #**********************---------------build transition and emission matrix------------------------************************************\n",
    "            \n",
    "            #bigram_matrix : count of every bigram tags\n",
    "            #transmission_matrix : probability of every bigram tags\n",
    "            #emission_count_matrix : given a word, what is the count of each tag that it can have\n",
    "            #emission_matrix : given a word, what is the probability of tag\n",
    "            model.create_transition_matrix(train_data)\n",
    "            model.create_emission_matrix(train_data)\n",
    "\n",
    "            #**********************---------------------------------------************************************\n",
    "            \n",
    "            #**********************---------------initialize confustion matrix------------------------************************************\n",
    "            #confusion_matrix : A matrix with rows having actual tag and columns showing predicted tag\n",
    "            model.confusion_matrix=[]\n",
    "            for j in range(len(model.count_of_each_tag)):\n",
    "                model.confusion_matrix.append([0]*len(model.count_of_each_tag))\n",
    "            #**********************---------------------------------------************************************\n",
    "\n",
    "\n",
    "            #**********************---------------bulding confustion matrix------------------------************************************\n",
    "            coun=0\n",
    "            for sen in test_data:\n",
    "                coun+=1\n",
    "                tags_output = model.HMM_logic(remove_tags(sen))\n",
    "                model.build_confusion_matrix(sen,tags_output)\n",
    "            #**********************---------------------------------------************************************\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            #**********************---------------build masters------------------------************************************\n",
    "            for i in range(len(model.count_of_each_tag)):\n",
    "                row = model.idx_to_tag[i]\n",
    "                for j in range(len(self.count_of_each_tag)):\n",
    "                    col = model.idx_to_tag[j]\n",
    "                    master_transition_matrix[self.tag_to_idx[row]][self.tag_to_idx[col]] += model.transition_matrix[i][j]\n",
    "                    master_confusion_matrix[self.tag_to_idx[row]][self.tag_to_idx[col]] += model.confusion_matrix[i][j]\n",
    "            \n",
    "            for i in range(len(model.word_to_idx)):\n",
    "                row = model.idx_to_word[i]\n",
    "                for j in range(len(self.count_of_each_tag)):\n",
    "                    col = model.idx_to_tag[j]\n",
    "                    master_emission_matrix[self.word_to_idx[row]][self.tag_to_idx[col]] += model.emission_matrix[i][j]\n",
    "            \n",
    "            #**********************---------------------------------------*******************************************\n",
    "                \n",
    "\n",
    "        self.confusion_matrix = np.array(master_confusion_matrix)/k\n",
    "        self.transition_matrix=np.array(master_transition_matrix)/k\n",
    "        self.emission_matrix=np.array(master_emission_matrix)/k\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking data from corpus\n",
    "sentences_original = brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM_model()\n",
    "model.train_model(sentences_original, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'NOUN', 'NOUN', '<end>']\n"
     ]
    }
   ],
   "source": [
    "sent = [\"<start>\", \"boy\", \"laugh\", \"<end>\"]\n",
    "# sent = add_start_end_tag(sent)\n",
    "print(model.HMM_logic(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('Twitter_chatGPT_tagged_sentences.txt', 'r') as file:\n",
    "    # Read all lines and store them in a list\n",
    "    lines = file.readlines()\n",
    "for i in range(len(lines)):\n",
    "    lines[i] = lines[i].replace('\\\\','')\n",
    "# lines[13] = lines[13].replace('\\\\','')\n",
    "lines[6] = lines[6].replace('â€™', '')\n",
    "lines[7] = lines[7].replace('â€™', '')\n",
    "twitter_data = []\n",
    "for string_data in lines:\n",
    "    # Original string with problematic characters\n",
    "    # string_data = \"[('<start>', '<start>'), ('Whoâ€™', 'X'), ('is', 'VERB'), ('predicted', 'VERB'), ('to', 'ADP'), ('remain', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('top', 'NOUN'), ('10', 'NUM'), ('of', 'ADP'), ('global', 'ADJ'), ('200', 'NUM'), ('so', 'ADV'), ('please', 'ADV'), ('letâ€™s', 'VERB'), ('get', 'VERB'), ('those', 'DET'), ('streams', 'NOUN'), ('back', 'ADV'), ('to', 'ADP'), ('keep', 'VERB'), ('it', 'PRON'), ('stable', 'ADJ'), ('cuz', 'X'), ('we', 'PRON'), ('dropped', 'VERB'), (',', '.'), ('focus', 'VERB'), ('on', 'ADP'), ('Apple', 'PROPN'), ('Music', 'PROPN'), ('as', 'ADP'), ('well', 'ADV'), ('<end>', '<end>')]\"\n",
    "\n",
    "    # Remove the outer brackets and split the string into individual tuples\n",
    "    tuple_strings = string_data[1:-1].split(\"), (\")\n",
    "\n",
    "    # List to store the parsed tuples\n",
    "    tuple_list = []\n",
    "\n",
    "    # Iterate through each tuple string\n",
    "    for tuple_str in tuple_strings:\n",
    "        # Remove any quotes and spaces\n",
    "        tuple_str = tuple_str.strip(\"()\")\n",
    "        # Split the tuple into its two elements\n",
    "        elements = re.split(r\",\\s*(?=['\\\"])\", tuple_str)  # Split on comma followed by space and quote\n",
    "        # Clean up each element and remove surrounding quotes\n",
    "        element_1 = elements[0].strip().strip(\"'\\\"\")\n",
    "        element_2 = elements[1].strip().strip(\"'\\\"\")\n",
    "        # Append as a tuple to the list\n",
    "        tuple_list.append((element_1, element_2))\n",
    "\n",
    "\n",
    "    # Output the result\n",
    "    # print(tuple_list)\n",
    "    data =  [t for t in tuple_list if t != ('', '')]\n",
    "    # print(data)\n",
    "    twitter_data.append(data)\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(twitter_data)):\n",
    "    t1 = twitter_data[i][len(twitter_data[i])-1][0]\n",
    "    t2 = twitter_data[i][len(twitter_data[i])-1][1].replace(\"')]\", \"\")\n",
    "    twitter_data[i].pop(len(twitter_data[i])-1)\n",
    "    twitter_data[i].append((t1,t2))\n",
    "    twitter_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = []\n",
    "for sentence in twitter_data:\n",
    "    ls = []\n",
    "    sent = remove_tags(sentence)\n",
    "    predict = model.HMM_logic(sent)\n",
    "    # print(sentence)\n",
    "    for i in range(len(predict)):\n",
    "        # print(predict[i])\n",
    "        ls.append((sent[i], predict[i]))\n",
    "    predicted_data.append(ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<start>', '<start>'), ('It', 'PRON'), ('is', 'VERB'), ('with', 'ADP'), ('immeasurable', 'ADJ'), ('grief', 'NOUN'), ('that', 'ADP'), ('we', 'PRON'), ('confirm', 'VERB'), ('the', 'DET'), ('passing', 'NOUN'), ('of', 'ADP'), ('Chadwick', 'PROPN'), ('Boseman', 'PROPN'), ('.', '.'), ('Chadwick', 'PROPN'), ('was', 'VERB'), ('diagnosed', 'VERB'), ('with', 'ADP'), ('stage', 'NOUN'), ('III', 'NUM'), ('colon', 'NOUN'), ('cancer', 'NOUN'), ('in', 'ADP'), ('2016', 'NUM'), ('and', 'CONJ'), ('battled', 'VERB'), ('with', 'ADP'), ('it', 'PRON'), ('these', 'DET'), ('last', 'ADJ'), ('4', 'NUM'), ('years', 'NOUN'), ('as', 'ADP'), ('it', 'PRON'), ('progressed', 'VERB'), ('to', 'ADP'), ('stage', 'NOUN'), ('IV', 'NUM'), ('.', '.'), ('A', 'DET'), ('true', 'ADJ'), ('fighter', 'NOUN'), ('Chadwick', 'PROPN'), ('persevered', 'VERB'), ('through', 'ADP'), ('it', 'PRON'), ('all', 'DET'), ('and', 'CONJ'), ('brought', 'VERB'), ('you', 'PRON'), ('many', 'DET'), ('of', 'ADP'), ('the', 'DET'), ('films', 'NOUN'), ('you', 'PRON'), ('have', 'VERB'), ('come', 'VERB'), ('to', 'ADP'), ('love', 'VERB'), ('so', 'ADV'), ('much', 'ADV'), ('.', '.'), ('From', 'ADP'), ('Marshall', 'PROPN'), ('to', 'ADP'), ('Da', 'PROPN'), ('5', 'NUM'), ('Bloods', 'PROPN'), ('August', 'PROPN'), (\"Wilson's\", 'PROPN'), ('Ma', 'PROPN'), (\"Rainey's\", 'PROPN'), ('Black', 'PROPN'), ('Bottom', 'PROPN'), ('and', 'CONJ'), ('several', 'DET'), ('more', 'ADJ'), ('all', 'DET'), ('were', 'VERB'), ('filmed', 'VERB'), ('during', 'ADP'), ('and', 'CONJ'), ('between', 'ADP'), ('countless', 'ADJ'), ('surgeries', 'NOUN'), ('and', 'CONJ'), ('chemotherapy', 'NOUN'), ('.', '.'), ('It', 'PRON'), ('was', 'VERB'), ('the', 'DET'), ('honor', 'NOUN'), ('of', 'ADP'), ('his', 'PRON'), ('career', 'NOUN'), ('to', 'ADP'), ('bring', 'VERB'), ('King', 'PROPN'), (\"T'Challa\", 'PROPN'), ('to', 'ADP'), ('life', 'NOUN'), ('in', 'ADP'), ('Black', 'PROPN'), ('Panther', 'PROPN'), ('.', '.'), ('He', 'PRON'), ('died', 'VERB'), ('in', 'ADP'), ('his', 'PRON'), ('home', 'NOUN'), ('with', 'ADP'), ('his', 'PRON'), ('wife', 'NOUN'), ('and', 'CONJ'), ('family', 'NOUN'), ('by', 'ADP'), ('his', 'PRON'), ('side', 'NOUN'), ('.', '.'), ('The', 'DET'), ('family', 'NOUN'), ('thanks', 'VERB'), ('you', 'PRON'), ('for', 'ADP'), ('your', 'PRON'), ('love', 'NOUN'), ('and', 'CONJ'), ('prayers', 'NOUN'), ('and', 'CONJ'), ('asks', 'VERB'), ('that', 'ADP'), ('you', 'PRON'), ('continue', 'VERB'), ('to', 'ADP'), ('respect', 'VERB'), ('their', 'PRON'), ('privacy', 'NOUN'), ('during', 'ADP'), ('this', 'DET'), ('difficult', 'ADJ'), ('time', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Invenergy', 'PROPN'), ('commences', 'VERB'), ('operations', 'NOUN'), ('at', 'ADP'), ('10th', 'NUM'), ('Arizona', 'PROPN'), ('battery', 'NOUN'), ('energy', 'NOUN'), ('storage', 'NOUN'), ('project', 'NOUN'), (':', '.'), ('A', 'DET'), ('new', 'ADJ'), ('50', 'NUM'), ('MW', 'X'), ('/', 'X'), ('200', 'NUM'), ('MWh', 'X'), ('BESS', 'X'), ('in', 'ADP'), ('Arizona', 'PROPN'), ('brings', 'VERB'), ('a', 'DET'), ('milestone', 'NOUN'), ('for', 'ADP'), ('Invenergy', 'PROPN'), ('in', 'ADP'), ('one', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('United', 'PROPN'), ('States', 'PROPN'), ('most', 'ADV'), ('active', 'ADJ'), ('regions', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Strongest', 'ADJ'), ('Start', 'NOUN'), ('Ever', 'ADJ'), (':', '.'), ('Australian', 'ADJ'), ('ETF', 'X'), ('Market', 'NOUN'), ('breaks', 'VERB'), ('$', 'X'), ('200', 'NUM'), ('billion', 'NUM'), ('record', 'NOUN'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Manhattan', 'PROPN'), ('Associates', 'PROPN'), ('unveiled', 'VERB'), ('AI', 'X'), ('innovations', 'NOUN'), ('in', 'ADP'), ('supply', 'NOUN'), ('chain', 'NOUN'), ('planning', 'NOUN'), ('at', 'ADP'), ('#ManhattanExchange', 'X'), ('in', 'ADP'), ('Melbourne', 'PROPN'), ('attracting', 'VERB'), ('over', 'ADP'), ('200', 'NUM'), ('industry', 'NOUN'), ('representatives', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('The', 'DET'), ('government', 'NOUN'), ('will', 'VERB'), ('revive', 'VERB'), ('a', 'DET'), ('200-billion-baht', 'NUM'), ('water', 'NOUN'), ('management', 'NOUN'), ('project', 'NOUN'), ('to', 'ADP'), ('mitigate', 'VERB'), ('severe', 'ADJ'), ('flooding', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('Yom', 'PROPN'), ('River', 'PROPN'), ('basin', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('North', 'NOUN'), ('amid', 'ADP'), ('calls', 'NOUN'), ('to', 'ADP'), ('revisit', 'VERB'), ('the', 'DET'), ('controversial', 'ADJ'), ('Kaeng', 'PROPN'), ('Sua', 'PROPN'), ('Ten', 'PROPN'), ('Dam', 'PROPN'), ('plan', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('If', 'ADP'), ('you', 'PRON'), ('think', 'VERB'), ('about', 'ADP'), ('it', 'PRON'), ('artists', 'NOUN'), ('their', 'PRON'), ('lives', 'NOUN'), ('have', 'VERB'), ('no', 'DET'), ('meaning', 'NOUN'), ('for', 'ADP'), ('anyone', 'PRON'), ('.', '.'), ('Everyone', 'PRON'), ('talks', 'VERB'), ('about', 'ADP'), ('their', 'PRON'), ('works', 'NOUN'), ('sometimes', 'ADV'), ('200', 'NUM'), ('years', 'NOUN'), ('later', 'ADV'), ('.', '.'), ('As', 'ADP'), ('I', 'PRON'), ('said', 'VERB'), ('before', 'ADP'), ('the', 'DET'), ('author', 'NOUN'), ('is', 'VERB'), ('just', 'ADV'), ('a', 'DET'), ('gatekeeper', 'NOUN'), ('he', 'PRON'), ('is', 'VERB'), ('not', 'ADV'), ('important', 'ADJ'), ('for', 'ADP'), ('anyone', 'PRON'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Who', 'X'), ('is', 'VERB'), ('predicted', 'VERB'), ('to', 'ADP'), ('remain', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('top', 'NOUN'), ('10', 'NUM'), ('of', 'ADP'), ('global', 'ADJ'), ('200', 'NUM'), ('so', 'ADV'), ('please', 'ADV'), ('lets', 'VERB'), ('get', 'VERB'), ('those', 'DET'), ('streams', 'NOUN'), ('back', 'ADV'), ('to', 'ADP'), ('keep', 'VERB'), ('it', 'PRON'), ('stable', 'ADJ'), ('cuz', 'X'), ('we', 'PRON'), ('dropped', 'VERB'), ('focus', 'VERB'), ('on', 'ADP'), ('Apple', 'PROPN'), ('Music', 'PROPN'), ('as', 'ADP'), ('well', 'ADV'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('She', 'PRON'), ('said', 'VERB'), ('shell', 'VERB'), ('give', 'VERB'), ('200', 'NUM'), ('for', 'ADP'), ('him', 'PRON'), ('to', 'ADP'), ('fix', 'VERB'), ('it', 'PRON'), ('this', 'DET'), ('dude', 'NOUN'), ('is', 'VERB'), ('milking', 'VERB'), ('this', 'DET'), ('whole', 'ADJ'), ('situation', 'NOUN'), ('bro', 'X'), ('bouta', 'X'), ('come', 'VERB'), ('back', 'ADV'), ('and', 'CONJ'), ('offer', 'VERB'), ('300', 'NUM'), ('to', 'ADP'), ('just', 'ADV'), ('make', 'VERB'), ('it', 'PRON'), ('blow', 'VERB'), ('up', 'ADV'), ('when', 'ADP'), ('she', 'PRON'), ('turns', 'VERB'), ('it', 'PRON'), ('on', 'ADP'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Thank', 'VERB'), ('you', 'PRON'), ('to', 'ADP'), ('everyone', 'PRON'), ('we', 'PRON'), ('have', 'VERB'), ('worked', 'VERB'), ('with', 'ADP'), ('this', 'DET'), ('summer', 'NOUN'), ('alongside', 'ADP'), ('@CuffeandTaylor5', 'X'), ('venues', 'NOUN'), (':', '.'), ('@LythamFestival', 'X'), ('@ThePieceHall', 'X'), ('@ScarboroughOAT', 'X'), ('@DEPOTLIVE1', 'X'), ('at', 'ADP'), ('Cardiff', 'PROPN'), ('Castle', 'PROPN'), ('@llangollen_Eist', 'X'), ('84', 'NUM'), ('headline', 'NOUN'), ('shows', 'NOUN'), ('200+', 'NUM'), ('artists', 'NOUN'), ('Letâ€™s', 'VERB'), ('get', 'VERB'), ('ready', 'ADJ'), ('for', 'ADP'), ('2025', 'NUM'), ('!', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Kudos', 'NOUN'), ('to', 'ADP'), ('PreetiPal', 'PROPN'), ('on', 'ADP'), ('her', 'PRON'), ('second', 'ADJ'), ('medal', 'NOUN'), ('win', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('ParisParalympics2024', 'X'), ('.', '.'), ('Congratulations', 'NOUN'), ('on', 'ADP'), ('this', 'DET'), ('historic', 'ADJ'), ('win', 'NOUN'), ('at', 'ADP'), (\"Women's\", 'PROPN'), ('200', 'NUM'), ('m', 'X'), ('-', 'X'), ('T35', 'X'), ('Final', 'NOUN'), ('representing', 'VERB'), ('Bharat', 'PROPN'), ('.', '.'), ('Best', 'ADJ'), ('wishes', 'NOUN'), ('for', 'ADP'), ('her', 'PRON'), ('shining', 'ADJ'), ('career', 'NOUN'), ('ahead', 'ADV'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Celebrating', 'VERB'), ('25', 'NUM'), ('years', 'NOUN'), ('of', 'ADP'), ('legacy', 'NOUN'), ('we', 'PRON'), ('aim', 'VERB'), ('to', 'ADP'), ('quadruple', 'VERB'), ('net', 'ADJ'), ('profits', 'NOUN'), ('to', 'ADP'), ('â‚¹200', 'NUM'), ('crore', 'NOUN'), ('by', 'ADP'), ('FY27', 'NOUN'), ('.', '.'), ('Our', 'PRON'), ('CEO', 'NOUN'), ('J', 'NOUN'), ('Gopalakrishnan', 'NOUN'), ('unveiled', 'VERB'), ('SPLNxT', 'NOUN'), ('targeting', 'VERB'), ('â‚¹5,000', 'NUM'), ('crore', 'NOUN'), ('in', 'ADP'), ('sales', 'NOUN'), ('and', 'CONJ'), ('8-9', 'NUM'), ('million', 'NUM'), ('sq.', 'NOUN'), ('ft.', 'NOUN'), ('annually', 'ADV'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('For', 'ADP'), ('the', 'DET'), ('first', 'ADJ'), ('time', 'NOUN'), ('the', 'DET'), ('public', 'NOUN'), ('include', 'VERB'), ('Â£200/yr', 'NUM'), ('for', 'ADP'), ('health', 'NOUN'), ('costs', 'NOUN'), ('like', 'ADP'), ('private', 'ADJ'), ('dentistry', 'NOUN'), ('physio', 'NOUN'), ('&', 'CONJ'), ('counselling', 'NOUN'), ('because', 'ADP'), ('they', 'PRON'), ('no', 'ADV'), ('longer', 'ADV'), ('think', 'VERB'), ('it', 'PRON'), ('â€™s', 'PRON'), ('a', 'DET'), ('given', 'ADJ'), ('you', 'PRON'), ('can', 'VERB'), ('access', 'VERB'), ('them', 'PRON'), ('in', 'ADP'), ('good', 'ADJ'), ('time', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('NHS', 'NOUN'), ('.', '.'), ('And', 'CONJ'), ('those', 'PRON'), ('on', 'ADP'), ('low', 'ADJ'), ('incomes', 'NOUN'), ('are', 'VERB'), ('bang', 'ADV'), ('out', 'ADP'), ('of', 'ADP'), ('luck', 'NOUN'), ('because', 'ADP'), ('â€¦', '.')]\n",
      "[('<start>', '<start>'), ('Chelsea', 'NOUN'), ('topped', 'VERB'), ('the', 'DET'), ('Premier', 'NOUN'), ('League', 'NOUN'), ('summer', 'ADJ'), ('spending', 'NOUN'), ('table', 'NOUN'), ('with', 'ADP'), ('a', 'DET'), ('whopping', 'ADJ'), ('Â£200.8m', 'NUM'), ('signing', 'VERB'), ('11', 'NUM'), ('new', 'ADJ'), ('players', 'NOUN'), ('including', 'VERB'), ('Pedro', 'NOUN'), ('Neto', 'NOUN'), ('(Â£50.5m', 'NUM'), (')', '.'), ('and', 'CONJ'), ('Jadon', 'NOUN'), ('Sancho', 'NOUN'), ('on', 'ADP'), ('loan', 'NOUN'), ('.', '.'), ('Tottenham', 'NOUN'), ('followed', 'VERB'), ('with', 'ADP'), ('Â£125.2m', 'NUM'), ('while', 'CONJ'), ('Arsenal', 'NOUN'), ('spent', 'VERB'), ('Â£91.6m', 'NUM'), ('ranking', 'VERB'), ('them', 'PRON'), ('in', 'ADP'), ('the', 'DET'), ('middle', 'ADJ'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('In', 'ADP'), ('the', 'DET'), ('light', 'NOUN'), ('of', 'ADP'), ('reports', 'NOUN'), ('of', 'ADP'), ('inappropriate', 'ADJ'), ('kannada', 'NOUN'), ('translation', 'NOUN'), ('of', 'ADP'), ('questions', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('KPSC', 'NOUN'), ('Gazetted', 'NOUN'), ('Probationers', 'NOUN'), ('', 'PRON'), ('examination', 'NOUN'), ('I', 'PRON'), ('have', 'VERB'), ('directed', 'VERB'), ('KPSC', 'NOUN'), ('to', 'ADP'), ('reconduct', 'VERB'), ('of', 'ADP'), ('the', 'DET'), ('examination', 'NOUN'), ('within', 'ADP'), ('2', 'NUM'), ('months', 'NOUN'), ('to', 'ADP'), ('ensure', 'VERB'), ('fairness', 'NOUN'), ('to', 'ADP'), ('all', 'DET'), ('candidates', 'NOUN'), ('.', '.'), ('Those', 'DET'), ('responsible', 'ADJ'), ('for', 'ADP'), ('these', 'DET'), ('lapses', 'NOUN'), ('have', 'VERB'), ('been', 'VERB'), ('relieved', 'VERB'), ('of', 'ADP'), ('their', 'PRON'), ('duties', 'NOUN'), ('.', '.'), ('The', 'DET'), ('upcoming', 'ADJ'), ('examination', 'NOUN'), ('will', 'VERB'), ('be', 'VERB'), ('conducted', 'VERB'), ('with', 'ADP'), ('the', 'DET'), ('utmost', 'ADJ'), ('responsibility', 'NOUN'), ('and', 'CONJ'), ('accountability', 'NOUN'), ('following', 'VERB'), ('all', 'DET'), ('due', 'ADJ'), ('diligence', 'NOUN'), ('.', '.'), ('We', 'PRON'), ('remain', 'VERB'), ('committed', 'VERB'), ('to', 'ADP'), ('upholding', 'VERB'), ('the', 'DET'), ('integrity', 'NOUN'), ('of', 'ADP'), ('our', 'PRON'), ('recruitment', 'NOUN'), ('processes', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('I', 'PRON'), ('think', 'VERB'), ('he', 'PRON'), ('will', 'VERB'), ('come', 'VERB'), ('out', 'ADP'), ('and', 'CONJ'), ('they', 'PRON'), ('may', 'VERB'), ('yet', 'ADV'), ('make', 'VERB'), ('a', 'DET'), ('game', 'NOUN'), ('out', 'ADP'), ('of', 'ADP'), ('this', 'DET'), ('if', 'ADP'), ('they', 'PRON'), ('get', 'VERB'), ('to', 'ADP'), ('200', 'NUM'), ('but', 'CONJ'), ('the', 'DET'), ('inept', 'ADJ'), ('batting', 'NOUN'), ('is', 'VERB'), ('leaving', 'VERB'), ('too', 'ADV'), ('much', 'ADV'), ('to', 'ADP'), ('do', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('final', 'ADJ'), ('analysis', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('On', 'ADP'), ('no.2', 'NUM'), ('Aggregate', 'NOUN'), ('Demand', 'NOUN'), ('also', 'ADV'), ('fell', 'VERB'), ('in', 'ADP'), ('Developed', 'ADJ'), ('Countries', 'NOUN'), ('during', 'ADP'), ('Covid', 'NOUN'), ('&', 'CONJ'), ('after', 'ADP'), ('!', 'PUNCT'), ('I', 'PRON'), ('ve', 'VERB'), ('never', 'ADV'), ('had', 'VERB'), ('a', 'DET'), ('problem', 'NOUN'), ('with', 'ADP'), ('Debt', 'NOUN'), ('as', 'ADP'), ('long', 'ADJ'), ('as', 'ADP'), ('it', 'PRON'), ('s', 'VERB'), ('really', 'ADV'), ('used', 'VERB'), ('to', 'ADP'), ('develop', 'VERB'), ('the', 'DET'), ('country', 'NOUN'), ('&', 'CONJ'), ('not', 'ADV'), ('corrupted', 'VERB'), ('by', 'ADP'), ('the', 'DET'), ('elites', 'NOUN'), ('!', 'PUNCT'), ('But', 'CONJ'), ('of', 'ADP'), ('course', 'NOUN'), ('not', 'ADV'), ('200%', 'NUM'), ('debt', 'NOUN'), ('to', 'ADP'), ('GDP', 'NOUN'), ('ratio', 'NOUN'), ('!', 'PUNCT'), ('You', 'PRON'), ('re', 'VERB'), ('an', 'DET'), ('IMF', 'NOUN'), ('analyst', 'NOUN'), (\"can't\", 'VERB'), ('be', 'VERB'), ('trusted', 'VERB'), ('!', 'PUNCT'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Pakistan', 'NOUN'), ('might', 'VERB'), ('have', 'VERB'), ('been', 'VERB'), ('bowled', 'VERB'), ('out', 'ADP'), ('by', 'ADP'), ('now', 'ADV'), ('if', 'ADP'), ('Rizwan', 'NOUN'), ('hadnâ€™t', 'VERB'), ('been', 'VERB'), ('dropped', 'VERB'), ('on', 'ADP'), ('0', 'NUM'), ('.', '.'), ('Heâ€™s', 'PRON'), ('now', 'ADV'), ('on', 'ADP'), ('38*', 'NUM'), ('and', 'CONJ'), ('looking', 'VERB'), ('to', 'ADP'), ('score', 'VERB'), ('runs', 'NOUN'), ('quickly', 'ADV'), ('.', '.'), ('We', 'PRON'), ('need', 'VERB'), ('to', 'ADP'), ('stop', 'VERB'), ('him', 'PRON'), ('before', 'ADP'), ('they', 'PRON'), ('reach', 'VERB'), ('200', 'NUM'), ('.', '.'), ('If', 'ADP'), ('they', 'PRON'), ('set', 'VERB'), ('us', 'PRON'), ('a', 'DET'), ('target', 'NOUN'), ('of', 'ADP'), ('230/250', 'NUM'), ('then', 'ADV'), ('we', 'PRON'), ('â€™re', 'VERB'), ('in', 'ADP'), ('trouble', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Yes', 'ADV'), ('.', '.'), ('Agree', 'VERB'), ('.', '.'), ('But', 'CONJ'), ('government', 'NOUN'), ('must', 'AUX'), ('do', 'VERB'), ('it', 'PRON'), ('with', 'ADP'), ('sensibility', 'NOUN'), ('knowledge', 'NOUN'), ('and', 'CONJ'), ('understanding', 'NOUN'), ('their', 'PRON'), ('decisions', 'NOUN'), ('.', '.'), ('And', 'CONJ'), ('many', 'ADJ'), ('of', 'ADP'), ('those', 'DET'), ('wishes', 'NOUN'), ('cannot', 'AUX'), ('be', 'VERB'), ('implemented', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('middle', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('fiscal', 'ADJ'), ('year', 'NOUN'), ('.', '.'), ('Sadly', 'ADV'), ('torries', 'NOUN'), ('left', 'VERB'), ('us', 'PRON'), ('all', 'ADV'), ('with', 'ADP'), ('many', 'ADJ'), ('problems', 'NOUN'), ('which', 'PRON'), ('cannot', 'AUX'), ('be', 'VERB'), ('sorted', 'VERB'), ('in', 'ADP'), ('100', 'NUM'), ('or', 'CONJ'), ('even', 'ADV'), ('200', 'NUM'), ('days', 'NOUN'), ('after', 'ADP'), ('elections', 'NOUN'), ('Sadly', 'ADV'), ('we', 'PRON'), ('will', 'AUX'), ('still', 'ADV'), ('pay', 'VERB'), ('for', 'ADP'), ('the', 'DET'), ('decisions', 'NOUN'), ('in', 'ADP'), ('longer', 'ADJ'), ('term', 'NOUN'), ('and', 'CONJ'), ('even', 'ADV'), ('with', 'ADP'), ('shorter', 'ADJ'), ('this', 'DET'), ('Winter', 'NOUN'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('The', 'DET'), ('#China-built', 'ADJ'), ('Redstone', 'NOUN'), ('Concentrated', 'ADJ'), ('#Solar', 'ADJ'), ('Thermal', 'ADJ'), ('Power', 'NOUN'), ('Project', 'NOUN'), ('in', 'ADP'), ('south', 'ADJ'), ('#Africa', 'NOUN'), ('the', 'DET'), ('first-of-its-kind', 'ADJ'), ('#RenewableEnergy', 'ADJ'), ('project', 'NOUN'), ('in', 'ADP'), ('sub-Saharan', 'ADJ'), ('Africa', 'NOUN'), ('will', 'AUX'), ('be', 'VERB'), ('connected', 'VERB'), ('to', 'ADP'), ('the', 'DET'), ('grid', 'NOUN'), ('in', 'ADP'), ('early', 'ADJ'), ('September', 'NOUN'), ('to', 'ADP'), ('supply', 'VERB'), ('480', 'NUM'), ('GWh', 'NOUN'), ('of', 'ADP'), ('#electricity', 'NOUN'), ('for', 'ADP'), ('200,000+', 'NUM'), ('households', 'NOUN'), ('annually', 'ADV'), ('.', '.'), ('<end>', '<end>')]\n",
      "[('<start>', '<start>'), ('Meet', 'VERB'), ('Teresa', 'NOUN'), ('who', 'PRON'), ('supports', 'VERB'), ('@WFP', 'NOUN'), ('funded', 'VERB'), ('food', 'NOUN'), ('distributions', 'NOUN'), ('.', '.'), ('In', 'ADP'), ('Unity', 'NOUN'), ('since', 'ADP'), ('January', 'NOUN'), ('we', 'PRON'), ('ve', 'VERB'), ('provided', 'VERB'), ('food', 'NOUN'), ('to', 'ADP'), ('200,000+', 'NUM'), ('residents', 'NOUN'), ('IDPs', 'NOUN'), ('and', 'CONJ'), ('returnees', 'NOUN'), ('fleeing', 'VERB'), ('#Sudan', 'NOUN'), ('.', '.'), ('Teresa', 'NOUN'), ('ensures', 'VERB'), ('the', 'DET'), ('most', 'ADV'), ('vulnerable', 'ADJ'), ('are', 'AUX'), ('reached', 'VERB'), ('and', 'CONJ'), ('their', 'PRON'), ('voices', 'NOUN'), ('heard', 'VERB'), ('.', '.'), ('<end>', '<end>')]\n"
     ]
    }
   ],
   "source": [
    "# predicted_data\n",
    "# twitter_data[0][len(twitter_data[0])-1][1]\n",
    "for i in twitter_data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"input.txt\", 'w') as file:\n",
    "#     for item in twitter_data:\n",
    "#         file.write(f\"{item}\\n\")\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"input.txt\", 'w') as file:\n",
    "    for item in twitter_data:\n",
    "        json.dump(item, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample list\n",
    "# my_list = ['apple', 'banana', 'cherry']\n",
    "\n",
    "# Open a file in write mode\n",
    "with open('twitter_predicted_data.txt', 'w') as file:\n",
    "    for item in predicted_data:\n",
    "        file.write(f\"{item}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_differences(list1, list2, filename, sample_number):\n",
    "    with open(filename, 'a') as file:\n",
    "        # Determine the maximum length of the lists\n",
    "        max_len = max(len(list1), len(list2))\n",
    "        file.write(f\"-----------------Showing differneces for sample {sample_number + 1}----------------\\n                 (our Model) vs (chatGPT)\\n\\n\")\n",
    "        for i in range(max_len):\n",
    "            elem1 = list1[i] if i < len(list1) else ''\n",
    "            elem2 = list2[i] if i < len(list2) else ''\n",
    "            \n",
    "            if elem1 != elem2:\n",
    "                # Highlight differences by marking with asterisks or other symbols\n",
    "                line = f\"Position {i + 1}: *{elem1}* vs *{elem2}*\\n\"\n",
    "            else:\n",
    "                line = f\"Position {i + 1}: {elem1} vs {elem2}\\n\"\n",
    "            \n",
    "            file.write(line)\n",
    "        file.write(\"\\n------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Example usage\n",
    "# list1 = ['apple', 'banana', 'cherry']\n",
    "# list2 = ['apple', 'blueberry', 'cherry']\n",
    "# predicted_data[0]\n",
    "twitter_data[0]\n",
    "for i in range(len(twitter_data)):\n",
    "    highlight_differences(predicted_data[i], twitter_data[i], f'highlighted_differences.txt', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40      40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# models=[]\n",
    "# for i in range(2,11):\n",
    "#     model=HMM_model()\n",
    "#     model.train_model(sentences_original, i)\n",
    "#     models.append(model)\n",
    "#     print(\"Accuracy of k = \", i,\" is = \", accuracy_calculator(model.confusion_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall :  [1.0, 0.9909063706493257, 0.9300074757401345, 0.8145029323586673, 0.905909712722298, 0.9531174447038668, 0.9994985260732557, 0.999598883850715, 0.8188267927950353, 0.990118214463579, 0.6401153240135438, 0.9382170511209308, 0.7786069651741292, 0.03391053391053392]\n",
      "[57340, 137019, 275558, 83721, 182750, 144766, 147565, 57340, 56239, 38151, 29829, 49334, 14874, 1386]\n"
     ]
    }
   ],
   "source": [
    "print(\"recall : \", recall(confusion_matrix))\n",
    "print(count_of_each_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  [1.0, 0.941530812257972, 0.8986341991959168, 0.879624516841524, 0.9668998128263225, 0.8679951446489986, 0.9779384563645536, 0.8877961901811987, 0.9195839391566939, 0.9899966655551851, 0.7854426002766252, 0.9751655629139073, 0.9314671814671814, 0.04]\n"
     ]
    }
   ],
   "source": [
    "print(\"precision : \", precision(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285327995284794\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_calculator(np_master_confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'<start>': 0, 'DET': 1, 'NOUN': 2, 'ADJ': 3, 'VERB': 4, 'ADP': 5, '.': 6, '<end>': 7, 'ADV': 8, 'CONJ': 9, 'PRT': 10, 'PRON': 11, 'NUM': 12, 'X': 13}\n"
     ]
    }
   ],
   "source": [
    "print(heatmap(np_master_confusion_matrix))\n",
    "print(master_tag_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'ADP', 'PRON', 'VERB', 'ADJ', 'VERB', '.', 'PRON', 'VERB', 'PRON', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "input_sentence1=[\"<start>\", \"Though\",\"he\",\"was\",\"good\",\"looking\",\",\",\"she\",\"rejected\",\"him\", \".\", \"<end>\"]\n",
    "tags_output= models[4].HMM_logic(input_sentence1)\n",
    "print(tags_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (34197387.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    print((s[i],tags_output[i]) for i in range len(s))\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "s = input().split(\" \")\n",
    "s=[\"<start>\"]+s+[\"<end>\"]\n",
    "print(s)\n",
    "tags_output= models[4].HMM_logic(s)\n",
    "p=[(s[i],tags_output[i]) for i in range(len(s))]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
