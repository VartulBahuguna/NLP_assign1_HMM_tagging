{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\VARTUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\VARTUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add start and end tags:\n",
    "def add_start_end_tag(sentences_original):\n",
    "    sentences=[\"\"]*len(sentences_original)\n",
    "    for i in range(len(sentences_original)):\n",
    "        sentences[i]=[(\"<start>\",\"<start>\")]+sentences_original[i]+[(\"<end>\",\"<end>\")]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(sentence):\n",
    "    new_sentence=[]\n",
    "    for i in sentence:\n",
    "        new_sentence.append(i[0])\n",
    "\n",
    "    # print(new_sentence)\n",
    "\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits data in k sets:\n",
    "def k_splits (k, data):\n",
    "    splits = {}\n",
    "    n = len(data)//k\n",
    "    for i in range(0, k):\n",
    "        i = int(i)\n",
    "        if(i+1 == k):\n",
    "            splits[i]= data[n*i : ]\n",
    "        else:\n",
    "            splits[i]=data[n*i : n*(i+1)]\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(confusion_matrix):\n",
    "    precision = []\n",
    "    n = len(confusion_matrix)\n",
    "    for i in range(0,n):\n",
    "        sum = 0\n",
    "        for j in range(0,len(confusion_matrix[i])):\n",
    "            sum+=confusion_matrix[j][i]\n",
    "        precision.append(confusion_matrix[i][i]/sum)\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(confusion_matrix):\n",
    "    recall = []\n",
    "    n = len(confusion_matrix)\n",
    "    for i in range(0,n):\n",
    "        sum = 0\n",
    "        for j in range(0,len(confusion_matrix[i])):\n",
    "            sum+=confusion_matrix[i][j]\n",
    "        recall.append(confusion_matrix[i][i]/sum)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calculator (confusion_matrix):\n",
    "    actual=0\n",
    "    count=0\n",
    "    for i in range(0,len(confusion_matrix)):\n",
    "        actual+=confusion_matrix[i][i]\n",
    "        count+=sum(confusion_matrix[i])\n",
    "    return (actual/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(confusion_matrix):\n",
    "\n",
    "    # Example data\n",
    "    data = confusion_matrix/np.sum(confusion_matrix,axis=1)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.imshow(data,  cmap='Spectral', interpolation='none')\n",
    "\n",
    "    # Add a colorbar\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add labels (optional)\n",
    "    plt.title('Heatmap Example')\n",
    "    plt.xlabel('X axis')\n",
    "    plt.ylabel('Y axis')\n",
    "    # Display the plot\n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_model:\n",
    "    def __init__(self):\n",
    "        self.confusion_matrix=None\n",
    "        self.transition_matrix=None\n",
    "        self.emission_matrix=None\n",
    "        self.tags_to_idx=None\n",
    "        self.count_of_each_tag=None\n",
    "        self.idx_to_tag=None\n",
    "        self.word_to_idx=None\n",
    "        self.idx_to_word=None\n",
    "    \n",
    "    def creat_tags_meta(self,sentences):\n",
    "        \n",
    "        #tag_to_idx: given a tag, what is its index : tags dict {\"tag\" : \"tag_index\"}\n",
    "        #idx_to_tag: given an index what is the tag : tags_reverse list of string\n",
    "        #count_of_each_tag: count of each tag\n",
    "        self.tag_to_idx={}\n",
    "        self.idx_to_tag=[]\n",
    "        cnt = 0\n",
    "        self.count_of_each_tag=[]\n",
    "        for i in range(len(sentences)):\n",
    "            for j in sentences[i]:\n",
    "                if j[1] not in self.tag_to_idx:\n",
    "                    self.tag_to_idx[j[1]] = cnt\n",
    "                    self.idx_to_tag.append(j[1])\n",
    "                    self.count_of_each_tag.append(0)\n",
    "                    cnt += 1\n",
    "                self.count_of_each_tag[self.tag_to_idx[j[1]]]+=1\n",
    "\n",
    "\n",
    "    def create_word_idx_translation(self,sentences):\n",
    "        #word to index number\n",
    "        #word_to_idx is word to index number\n",
    "        #idx_to_word\n",
    "        \n",
    "        idx = 0\n",
    "        self.word_to_idx={}\n",
    "        self.idx_to_word = []\n",
    "        # word_to_idx[\"<start>\"]=idx\n",
    "        for sentence in sentences:\n",
    "            for tup in sentence:\n",
    "                if tup[0] not in self.word_to_idx:\n",
    "                    \n",
    "                    self.word_to_idx[tup[0]]=idx\n",
    "                    idx += 1\n",
    "                    self.idx_to_word.append(tup[0])\n",
    "        self.word_to_idx[\"<unknown>\"]=idx\n",
    "        self.idx_to_word.append(\"<unknown>\")\n",
    "\n",
    "        # print('idx--------------------------------', idx)\n",
    "        # print('worddict--------------------------------', len(word_to_idx))\n",
    "\n",
    "\n",
    "    def create_transition_matrix(self,sentences):\n",
    "        #creating transition matrix\n",
    "        #bigram_matrix : count of every bigram tags\n",
    "        #transmission_matrix : probability of every bigram tags \n",
    "        self.bigram_matrix = []\n",
    "        self.transition_matrix = []\n",
    "        total_tags = len(self.count_of_each_tag)\n",
    "        for i in range(total_tags): \n",
    "            self.bigram_matrix.append([0]*total_tags)\n",
    "            self.transition_matrix.append([0]*total_tags)\n",
    "\n",
    "        #creating a bigram matrix\n",
    "        for sentence in sentences:\n",
    "            for j in range(len(sentence)-1):\n",
    "                self.bigram_matrix[self.tag_to_idx[sentence[j][1]]][self.tag_to_idx[sentence[j+1][1]]]+=1\n",
    "        # print(bigram_matrix)\n",
    "        \n",
    "        #create transition matrix A\n",
    "        for i in range(total_tags):\n",
    "            for j in range(total_tags):\n",
    "                self.transition_matrix[i][j]=(self.bigram_matrix[i][j]+1)/(self.count_of_each_tag[i] + len(self.count_of_each_tag))  #adding 1 to numerator and total unique tags in denominator for smoothing(Laplace)\n",
    "        # print(transition_matrix)\n",
    "\n",
    "\n",
    "    def create_emission_matrix(self,sentences):\n",
    "        #emission_matrix : given a word, what is the probability of tag\n",
    "        #emission_count_matrix : given a word, what is the count of each tag that it can have\n",
    "        # words = list(brown.words())\n",
    "        words_cnt = len(self.word_to_idx)\n",
    "        total_tags = len(self.tag_to_idx)\n",
    "        self.emission_matrix = []\n",
    "        \n",
    "        for i in range(words_cnt):\n",
    "            self.emission_matrix.append([0]*total_tags)\n",
    "        # print(\"--------------------------------------\",len(word_to_idx))\n",
    "        # print(\"------------------------\", len(emission_matrix))\n",
    "        # print(\"------------------------\", word_to_idx['<unknown>'])\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for j in sentence:\n",
    "                self.emission_matrix[self.word_to_idx[j[0]]][self.tag_to_idx[j[1]]]+=1\n",
    "\n",
    "        emission_count_matrix = self.emission_matrix\n",
    "\n",
    "        for i in range(words_cnt):\n",
    "            for j in range(total_tags):\n",
    "                self.emission_matrix[i][j] = (self.emission_matrix[i][j]+1)/(self.count_of_each_tag[j] + words_cnt)\n",
    "        # print(\"------------------------\", emission_matrix[word_to_idx['unknown'])\n",
    "\n",
    "        # handling missing case\n",
    "        aux=[0]*len(self.tag_to_idx.keys())\n",
    "        # print(tag_to_idx)\n",
    "        aux[self.tag_to_idx['NOUN']]=self.count_of_each_tag[self.tag_to_idx['NOUN']]/total_tags\n",
    "        aux[self.tag_to_idx['VERB']]=self.count_of_each_tag[self.tag_to_idx['VERB']]/total_tags\n",
    "        aux[self.tag_to_idx['ADV']]=self.count_of_each_tag[self.tag_to_idx['ADV']]/total_tags\n",
    "        aux[self.tag_to_idx['ADJ']]=self.count_of_each_tag[self.tag_to_idx['ADJ']]/total_tags\n",
    "        # print(aux)\n",
    "        self.emission_matrix[self.word_to_idx['<unknown>']]=aux\n",
    "\n",
    "\n",
    "    def build_confusion_matrix(self,actual,predicted):\n",
    "        #cm: confusion matrix\n",
    "        for i in range(len(actual)):\n",
    "            self.confusion_matrix[self.tag_to_idx[actual[i][1]]][self.tag_to_idx[predicted[i]]]+=1\n",
    "\n",
    "    def HMM_logic(self,input_sentence):\n",
    "        # print(tags_dict)\n",
    "        tags_output=[\"<start>\"]\n",
    "        last_prob = 1\n",
    "        viterbi=[]\n",
    "        for i in range(len(self.tag_to_idx)):\n",
    "            viterbi.append(([0])*len(input_sentence))\n",
    "        \n",
    "        viterbi[self.tag_to_idx[\"<start>\"]][0]=1\n",
    "        \n",
    "        for i in range(1, len(input_sentence)):\n",
    "            word_prob=0\n",
    "            tags=\"\"\n",
    "            curr_ob = input_sentence[i]\n",
    "            if curr_ob not in self.word_to_idx:\n",
    "                curr_ob=\"<unknown>\"\n",
    "            for curr_tag in self.tag_to_idx:\n",
    "                max_prob=0\n",
    "                for prev_tag in self.tag_to_idx:\n",
    "                    temp = self.transition_matrix[self.tag_to_idx[prev_tag]][self.tag_to_idx[curr_tag]]*self.emission_matrix[self.word_to_idx[curr_ob]][self.tag_to_idx[curr_tag]]*viterbi[self.tag_to_idx[prev_tag]][i-1]\n",
    "                    if temp>max_prob:\n",
    "                        max_prob=temp\n",
    "                viterbi[self.tag_to_idx[curr_tag]][i]=max_prob\n",
    "                if max_prob>word_prob:\n",
    "                    word_prob=max_prob\n",
    "                    tags=curr_tag\n",
    "            if tags=='':\n",
    "                tags='X'\n",
    "            tags_output.append(tags)\n",
    "\n",
    "        return tags_output  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self,sentences_original, k):\n",
    "\n",
    "        model=HMM_model()\n",
    "\n",
    "        sentences = add_start_end_tag(sentences_original)\n",
    "        \n",
    "        #spliting data into k splits\n",
    "        cross_validation_set=k_splits(k,sentences)\n",
    "        keys=list(cross_validation_set.keys())\n",
    "\n",
    "        #creating master meta data i.e. over whole corpus\n",
    "        self.creat_tags_meta(sentences)\n",
    "        self.create_word_idx_translation(sentences)\n",
    "        #instializing master transition, emission and confusion matrices\n",
    "        master_transition_matrix = []\n",
    "        master_emission_matrix = []\n",
    "        master_confusion_matrix = []\n",
    "\n",
    "        for i in range(len(self.count_of_each_tag)):\n",
    "            master_transition_matrix.append([0]*len(self.count_of_each_tag))\n",
    "            master_confusion_matrix.append([0]*len(self.count_of_each_tag))\n",
    "\n",
    "        for i in range(len(self.word_to_idx)):\n",
    "            master_emission_matrix.append([0]*len(self.count_of_each_tag))\n",
    "\n",
    "\n",
    "\n",
    "        #model building and traing on k sets\n",
    "        for i in keys:\n",
    "\n",
    "            #**********************------------------create test and train------------------************************************\n",
    "            train_data=[]\n",
    "            for j in cross_validation_set.keys():\n",
    "                if j!=i:\n",
    "                    train_data+=cross_validation_set[j]\n",
    "\n",
    "            test_data=cross_validation_set[i]\n",
    "            #**********************---------------------------------------************************************\n",
    "            \n",
    "\n",
    "            #**********************---------------create tags and words meta data------------------------************************************\n",
    "            model.creat_tags_meta(train_data)\n",
    "            model.create_word_idx_translation(train_data)\n",
    "            #**********************---------------------------------------************************************\n",
    "\n",
    "\n",
    "            #**********************---------------build transition and emission matrix------------------------************************************\n",
    "            \n",
    "            #bigram_matrix : count of every bigram tags\n",
    "            #transmission_matrix : probability of every bigram tags\n",
    "            #emission_count_matrix : given a word, what is the count of each tag that it can have\n",
    "            #emission_matrix : given a word, what is the probability of tag\n",
    "            model.create_transition_matrix(train_data)\n",
    "            model.create_emission_matrix(train_data)\n",
    "\n",
    "            #**********************---------------------------------------************************************\n",
    "            \n",
    "            #**********************---------------initialize confustion matrix------------------------************************************\n",
    "            #confusion_matrix : A matrix with rows having actual tag and columns showing predicted tag\n",
    "            model.confusion_matrix=[]\n",
    "            for j in range(len(model.count_of_each_tag)):\n",
    "                model.confusion_matrix.append([0]*len(model.count_of_each_tag))\n",
    "            #**********************---------------------------------------************************************\n",
    "\n",
    "\n",
    "            #**********************---------------bulding confustion matrix------------------------************************************\n",
    "            coun=0\n",
    "            for sen in test_data:\n",
    "                coun+=1\n",
    "                tags_output = model.HMM_logic(remove_tags(sen))\n",
    "                model.build_confusion_matrix(sen,tags_output)\n",
    "            #**********************---------------------------------------************************************\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            #**********************---------------build masters------------------------************************************\n",
    "            for i in range(len(model.count_of_each_tag)):\n",
    "                row = model.idx_to_tag[i]\n",
    "                for j in range(len(self.count_of_each_tag)):\n",
    "                    col = model.idx_to_tag[j]\n",
    "                    master_transition_matrix[self.tag_to_idx[row]][self.tag_to_idx[col]] += model.transition_matrix[i][j]\n",
    "                    master_confusion_matrix[self.tag_to_idx[row]][self.tag_to_idx[col]] += model.confusion_matrix[i][j]\n",
    "            \n",
    "            for i in range(len(model.word_to_idx)):\n",
    "                row = model.idx_to_word[i]\n",
    "                for j in range(len(self.count_of_each_tag)):\n",
    "                    col = model.idx_to_tag[j]\n",
    "                    master_emission_matrix[self.word_to_idx[row]][self.tag_to_idx[col]] += model.emission_matrix[i][j]\n",
    "            \n",
    "            #**********************---------------------------------------*******************************************\n",
    "                \n",
    "\n",
    "        self.confusion_matrix = np.array(master_confusion_matrix)/k\n",
    "        self.transition_matrix=np.array(master_transition_matrix)/k\n",
    "        self.emission_matrix=np.array(master_emission_matrix)/k\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking data from corpus\n",
    "sentences_original = brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM_model()\n",
    "model.train_model(sentences_original, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('Twitter_chatGPT_tagged_sentences.txt', 'r') as file:\n",
    "    # Read all lines and store them in a list\n",
    "    lines = file.readlines()\n",
    "for i in range(len(lines)):\n",
    "    lines[i] = lines[i].replace('\\\\','')\n",
    "# lines[13] = lines[13].replace('\\\\','')\n",
    "lines[6] = lines[6].replace('â€™', '')\n",
    "lines[7] = lines[7].replace('â€™', '')\n",
    "twitter_data = []\n",
    "for string_data in lines:\n",
    "    # Original string with problematic characters\n",
    "    # string_data = \"[('<start>', '<start>'), ('Whoâ€™', 'X'), ('is', 'VERB'), ('predicted', 'VERB'), ('to', 'ADP'), ('remain', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('top', 'NOUN'), ('10', 'NUM'), ('of', 'ADP'), ('global', 'ADJ'), ('200', 'NUM'), ('so', 'ADV'), ('please', 'ADV'), ('letâ€™s', 'VERB'), ('get', 'VERB'), ('those', 'DET'), ('streams', 'NOUN'), ('back', 'ADV'), ('to', 'ADP'), ('keep', 'VERB'), ('it', 'PRON'), ('stable', 'ADJ'), ('cuz', 'X'), ('we', 'PRON'), ('dropped', 'VERB'), (',', '.'), ('focus', 'VERB'), ('on', 'ADP'), ('Apple', 'PROPN'), ('Music', 'PROPN'), ('as', 'ADP'), ('well', 'ADV'), ('<end>', '<end>')]\"\n",
    "\n",
    "    # Remove the outer brackets and split the string into individual tuples\n",
    "    tuple_strings = string_data[1:-1].split(\"), (\")\n",
    "\n",
    "    # List to store the parsed tuples\n",
    "    tuple_list = []\n",
    "\n",
    "    # Iterate through each tuple string\n",
    "    for tuple_str in tuple_strings:\n",
    "        # Remove any quotes and spaces\n",
    "        tuple_str = tuple_str.strip(\"()\")\n",
    "        # Split the tuple into its two elements\n",
    "        elements = re.split(r\",\\s*(?=['\\\"])\", tuple_str)  # Split on comma followed by space and quote\n",
    "        # Clean up each element and remove surrounding quotes\n",
    "        element_1 = elements[0].strip().strip(\"'\\\"\")\n",
    "        element_2 = elements[1].strip().strip(\"'\\\"\")\n",
    "        # Append as a tuple to the list\n",
    "        tuple_list.append((element_1, element_2))\n",
    "\n",
    "\n",
    "    # Output the result\n",
    "    # print(tuple_list)\n",
    "    data =  [t for t in tuple_list if t != ('', '')]\n",
    "    # print(data)\n",
    "    twitter_data.append(data)\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = []\n",
    "for sentence in twitter_data:\n",
    "    ls = []\n",
    "    sent = remove_tags(sentence)\n",
    "    predict = model.HMM_logic(sent)\n",
    "    # print(sentence)\n",
    "    for i in range(len(predict)):\n",
    "        # print(predict[i])\n",
    "        ls.append((sent[i], predict[i]))\n",
    "    predicted_data.append(ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_data\n",
    "# twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample list\n",
    "# my_list = ['apple', 'banana', 'cherry']\n",
    "\n",
    "# Open a file in write mode\n",
    "with open('twitter_predicted_data.txt', 'w') as file:\n",
    "    for item in predicted_data:\n",
    "        file.write(f\"{item}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mHMM_model()\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy of k = \u001b[39m\u001b[38;5;124m\"\u001b[39m, i,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is = \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_calculator(model\u001b[38;5;241m.\u001b[39mconfusion_matrix))\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mHMM_model.train_model\u001b[1;34m(self, sentences_original, k)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m test_data:\n\u001b[0;32m    222\u001b[0m     coun\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 223\u001b[0m     tags_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHMM_logic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     model\u001b[38;5;241m.\u001b[39mbuild_confusion_matrix(sen,tags_output)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m#**********************---------------------------------------************************************\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m#**********************---------------build masters------------------------************************************\u001b[39;00m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mHMM_model.HMM_logic\u001b[1;34m(self, input_sentence)\u001b[0m\n\u001b[0;32m    136\u001b[0m max_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prev_tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_to_idx:\n\u001b[1;32m--> 138\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_matrix[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_to_idx[prev_tag]][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_to_idx[curr_tag]]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memission_matrix\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_idx[curr_ob]][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_to_idx[curr_tag]]\u001b[38;5;241m*\u001b[39mviterbi[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_to_idx[prev_tag]][i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temp\u001b[38;5;241m>\u001b[39mmax_prob:\n\u001b[0;32m    140\u001b[0m         max_prob\u001b[38;5;241m=\u001b[39mtemp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# models=[]\n",
    "# for i in range(2,11):\n",
    "#     model=HMM_model()\n",
    "#     model.train_model(sentences_original, i)\n",
    "#     models.append(model)\n",
    "#     print(\"Accuracy of k = \", i,\" is = \", accuracy_calculator(model.confusion_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall :  [1.0, 0.9909063706493257, 0.9300074757401345, 0.8145029323586673, 0.905909712722298, 0.9531174447038668, 0.9994985260732557, 0.999598883850715, 0.8188267927950353, 0.990118214463579, 0.6401153240135438, 0.9382170511209308, 0.7786069651741292, 0.03391053391053392]\n",
      "[57340, 137019, 275558, 83721, 182750, 144766, 147565, 57340, 56239, 38151, 29829, 49334, 14874, 1386]\n"
     ]
    }
   ],
   "source": [
    "print(\"recall : \", recall(confusion_matrix))\n",
    "print(count_of_each_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  [1.0, 0.941530812257972, 0.8986341991959168, 0.879624516841524, 0.9668998128263225, 0.8679951446489986, 0.9779384563645536, 0.8877961901811987, 0.9195839391566939, 0.9899966655551851, 0.7854426002766252, 0.9751655629139073, 0.9314671814671814, 0.04]\n"
     ]
    }
   ],
   "source": [
    "print(\"precision : \", precision(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285327995284794\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_calculator(np_master_confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'<start>': 0, 'DET': 1, 'NOUN': 2, 'ADJ': 3, 'VERB': 4, 'ADP': 5, '.': 6, '<end>': 7, 'ADV': 8, 'CONJ': 9, 'PRT': 10, 'PRON': 11, 'NUM': 12, 'X': 13}\n"
     ]
    }
   ],
   "source": [
    "print(heatmap(np_master_confusion_matrix))\n",
    "print(master_tag_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'ADP', 'PRON', 'VERB', 'ADJ', 'VERB', '.', 'PRON', 'VERB', 'PRON', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "input_sentence1=[\"<start>\", \"Though\",\"he\",\"was\",\"good\",\"looking\",\",\",\"she\",\"rejected\",\"him\", \".\", \"<end>\"]\n",
    "tags_output= models[4].HMM_logic(input_sentence1)\n",
    "print(tags_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (34197387.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    print((s[i],tags_output[i]) for i in range len(s))\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "s = input().split(\" \")\n",
    "s=[\"<start>\"]+s+[\"<end>\"]\n",
    "print(s)\n",
    "tags_output= models[4].HMM_logic(s)\n",
    "p=[(s[i],tags_output[i]) for i in range(len(s))]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
